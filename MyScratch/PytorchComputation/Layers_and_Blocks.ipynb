{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a net in a process-oriented way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0041,  0.1994, -0.0439, -0.3023,  0.1296, -0.2967, -0.1734,  0.0149,\n",
       "         -0.1510, -0.2858],\n",
       "        [ 0.0819, -0.0980, -0.0153, -0.0092,  0.0740, -0.3273, -0.4315,  0.0735,\n",
       "          0.0503, -0.1280]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10)) \n",
    "# no nn.Softmax() is required here since in computing loss, it is done automatically in nn.CrossEntropyLoss()\n",
    "x = torch.randn(2,20)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0', Linear(in_features=20, out_features=256, bias=True)),\n",
       "             ('1', ReLU()),\n",
       "             ('2', Linear(in_features=256, out_features=10, bias=True))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net._modules # thi is an OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.children of Sequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in Ch3, we define MLP as a class, and its `.init()` create each layer by `nn.Linear()`, `nn.ReLU()` and use `forward()` to specify how the quantities should be computed, so **essentially, object oriented way is a BLOCK!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MySequential, self).__init__(**kwargs)\n",
    "    \n",
    "    def add_module(self, block):\n",
    "        self._modules[block] = block # OrderedDict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self._modules.values():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core is the add method. It adds any block to the ordered dictionary of children. These are then executed in sequence when forward propagation is invoked. Letâ€™s see what the MLP looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0037,  0.2254, -0.3256,  0.0614, -0.0765,  0.2539, -0.0049, -0.1747,\n",
       "          0.2823, -0.0172],\n",
       "        [-0.0225,  0.0785, -0.0906, -0.0450, -0.1233,  0.1334, -0.0704, -0.3179,\n",
       "         -0.0714, -0.2188]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add_module(nn.Linear(20, 256))\n",
    "net.add_module(nn.ReLU())\n",
    "net.add_module(nn.Linear(256, 10))\n",
    "torch.manual_seed(100)\n",
    "x = torch.randn(2, 20)\n",
    "net(x) \n",
    "# fixme:\n",
    "# manually set seed does not remove the randomness in initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Executing Code in the Forward Propagation Function**]\n",
    "\n",
    "The `Sequential` class makes model construction easy,\n",
    "allowing us to assemble new architectures\n",
    "without having to define our own class.\n",
    "However, not all architectures are simple daisy chains.\n",
    "When greater flexibility is required,\n",
    "we will want to define our own blocks.\n",
    "For example, we might want to execute\n",
    "Python's control flow within the forward propagation function.\n",
    "Moreover, we might want to perform\n",
    "arbitrary mathematical operations,\n",
    "not simply relying on predefined neural network layers.\n",
    "\n",
    "You might have noticed that until now,\n",
    "all of the operations in our networks\n",
    "have acted upon our network's activations\n",
    "and its parameters.\n",
    "Sometimes, however, we might want to\n",
    "incorporate terms\n",
    "that are neither the result of previous layers\n",
    "nor updatable parameters.\n",
    "We call these *constant parameters*.\n",
    "Say for example that we want a layer\n",
    "that calculates the function\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$,\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{w}$ is our parameter,\n",
    "and $c$ is some specified constant\n",
    "that is not updated during optimization.\n",
    "So we implement a `FixedHiddenMLP` class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # Use the created constant parameters, as well as the `relu` and `mm`\n",
    "        # functions\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # Reuse the fully-connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully-connected layers\n",
    "        # TODO:\n",
    "        #   does reuse mean, the parameters of these two layers will remains the same? \n",
    "        #   I think so, since it calls the same method `self.linear()`\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc1b1c9b257fc8014d968e6a5c17774483cbc9201995df8261e8b19db7e7b46a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
